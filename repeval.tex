\documentclass[english,svgnames,notes=hide,14pt,aspectratio=169]{beamer}



\def\talktitle{How well do Embedding Models capture Non-compositionality? A View from Multiword Expressions}
\def\authorname{Navnita Nandakumar, Timothy Baldwin and Bahar Salehi}
\def\venuetitle{6/6/2019}


\usepackage{mybeamer}
\usepackage[normalem]{ulem}
\usepackage{pgfplots}
\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables


\usetheme[compress]{Ilmenau}
\makeatletter
\beamer@theme@subsectionfalse
\setbeamertemplate{footline}
{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=\paperwidth,ht=2.25ex,dp=1ex,center]{title 
in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
}%
  \vskip0pt%
}
\makeatother

\RequirePackage{tkz-macros}
%custom tikz styles
\tikzset{shaded-box/.style={rectangle, fill=gray,minimum size=20pt, 
inner sep=0pt}, alphabet/.style={rectangle, inner sep=5pt},
table/.style={circle,fill=white,draw=black,inner sep=1pt, minimum 
size=40pt, font=\fontsize{15}{10}\selectfont},
big/.style={rectangle, inner sep=0pt, font=\fontsize{15}{10}},
point/.style={rectangle, fill=black,minimum size=0pt, inner sep=0pt}}


%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------



\pgfplotstableread{
x   farahm  reddy   ramisch discoj
doc2vec .088  .025 .039    .003 
Infersent   .019    .527    .221    .202
fastText    .295    .285    .446    .353
ELMo    .283    .406    .486   .316
BERT    .193    .372    .107    .181
Flair   .23 .102   .311    .303
word2vec    .35 .634    .581    .427
}\dataset


\pgfplotstableread{
x   direct para combined
doc2vec .039    .388    .419
Infersent   .221    .712    .712
fastText    .446    .703    .703
ELMo    .459    .546    .546
BERT    .086    .583    .583
Flair   .295    .492    .492
word2vec    .581    .510    .677
}\datasetp


\newcommand{\lex}[1]{\textit{#1}\xspace}



\begin{document}

\begin{frame}
\titlepage % Print the title page as the first slide
\end{frame}


\begin{frame}
\frametitle{Multiword Expressions and Compositionality}


\begin{itemize}
\item A multiword expression (``MWE'') is an idiomatic combination of two or more
  words
\item Types of MWEs that are relevant to this talk:
  \begin{itemize}
  \item noun compounds: \lex{student loan, game plan}
  \item adjective--noun pairs: \lex{big idea, smart card}
  \end{itemize}
\item The (semantic) compositionality of an MWE is the degree to which
  its meaning can be derived from that of its components:
  \begin{itemize}
  \item For e.g., \lex{application form} is compositional, \lex{crocodile tears} is not
  \item Some MWEs might be semi-compositional, e.g. \lex{poor \uline{man}} and \lex{game \uline{plan}}
  \end{itemize}
\end{itemize}

\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Compositionality Prediction Techniques}
\textbf{Distributional Hypothesis}
\begin{itemize}
    \item \textit{``You shall know a word by the company it keeps''} (Firth 1957:p.11)
    \item The meaning of a word is predictable from its context
\end{itemize} \\
Distributional Similarity techniques operate under this hypothesis, assuming that an MWE must be compositional if it appears in similar contexts as its components.
\\~\\
\textbf{State-of-the-art:} The current best practice is to use \textbf{word2vec}, a word-level language embedding model, to measure the similarity between
the embedding of the MWE and the weighted sum of that of its constituents (Salehi et al. 2015; Cordeiro et al. 2019). \\
However, it requires token-level identification of the MWEs in the training corpus, which is not practical (knowledge of MWEs beforehand, retraining for new MWEs).
\end{frame}

%------------------------------------------------
\begin{frame}
\frametitle{Research Questions}
\begin{block}{Research Question #1}
Can modern language embedding techniques better predict compositionality?
\end{block} \\~\\
\begin{block}{Research Question #2}
Can paraphrase data enhance compositionality predictions?
\end{block}
\end{frame}
%------------------------------------------------

\begin{frame}
\frametitle{Language Embeddings}
Language Embeddings are multidimensional vector representations of text (a word, sentence or document), where each dimension represents a meaning and the text's association with it.
\\~\\
\textbf{Models used:}
\begin{itemize}
    \item \textbf{word2vec}: word-level embedding model, used for comparison
    \item \textbf{fastText}: uses sub-word information in the form of n-grams, allowing it to generate embeddings for OOVs
    \item \textbf{ELMo, BERT and Flair}: contextualised embeddings that capture word semantics in different contexts
    \item \textbf{doc2vec and Infersent}: document-level embeddings
\end{itemize}
%\begin{figure}
%\includegraphics[width=0.8\linewidth]{test}
%\end{figure}
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Approach}
\textbf{Datasets}: Normalised scores range from 0 - 100.
\begin{itemize}
    \item 3 English binary noun compound datasets: \textbf{Farahmand, Reddy and Ramisch}
    \item 1 English adjective-noun pair datasets: \textbf{Disco\textsubscript{adj}}
\end{itemize}
\textbf{Ramisch} also contains paraphrases of the MWE, listed in decreasing order of popularity.\\~\\

\textbf{Method}: Using \uline{pre-trained} models, we calculate the cosine similarity between the embedding of the MWE and
\begin{itemize}
    \item the weighted sum of that of its constituents \textit{(direct combination)}
    \item that of one/the sum/the average of its paraphrases \textit{(paraphrase metric)}
\end{itemize}
We also combine the two metrics to yield a third.
\end{frame}


\begin{frame}
\frametitle{Results - Overall Performance (Direct)}
\begin{center}
\pgfplotsset{width=14cm,height=6cm}
\begin{tikzpicture}
\begin{axis}[
    bar width=2mm,
    ybar,
    %enlargelimits=0.15,
    legend style={at={(0.5,-0.25)},
      anchor=north,legend columns=-1},
    ylabel={$r$},
    symbolic x coords={doc2vec, Infersent, fastText,ELMo,BERT, Flair, word2vec},
    xtick=data,
    xticklabel style = {rotate=15},
    ymajorgrids=true,
    %nodes near coords,
    %nodes near coords align={vertical},
    ]
\addplot[draw=blue!40,fill=blue!20] table[x=x,y=farahm] \dataset;
\addplot[draw=red!40,fill=red!40] table[x=x,y=ramisch] \dataset;
\addplot[draw=orange!40,fill=orange!40] table[x=x,y=reddy] \dataset;
\addplot[draw=green!40,fill=green!40] table[x=x,y=discoj] \dataset;
\legend{Farahmand, Ramisch, Reddy, Disco};
\end{axis}
\end{tikzpicture}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Results - Use of Paraphrases (Ramisch)}
\begin{center}
\pgfplotsset{width=12cm,height=6cm}
\begin{tikzpicture}
\begin{axis}[
    bar width=2mm,
    ybar,
    %enlargelimits=0.15,
    legend style={at={(0.5,-0.25)},
      anchor=north,legend columns=-1},
    ylabel={$r$},
    symbolic x coords={doc2vec, Infersent, fastText,ELMo,BERT, Flair, word2vec},
    xtick=data,
    xticklabel style = {rotate=10},
    ymajorgrids=true,
    %nodes near coords,
    %nodes near coords align={vertical},
    ]
\addplot[draw=red!40,fill=red!40] table[x=x,y=direct] \datasetp;
\addplot[draw=orange!40,fill=orange!40] table[x=x,y=para] \datasetp;
\addplot[draw=blue!40,fill=blue!40] table[x=x,y=combined] \datasetp;

\legend{Direct Combination, Paraphrases, Combined};
\end{axis}
\end{tikzpicture}
\end{center}
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Observations}
\begin{itemize}
    \item None of the modern embedding models were able to outperform word2vec in the direct combination approach \\~\\
    \item Infersent and fastText have the best results for the paraphrase metric \\~\\
    \item The contextualised embeddings did not perform as well as expected; their raw usage without context provided similar results\\~\\
    \item Overall, the methods struggle to model highly non-compositional MWEs, as well as the more dated MWEs (\textit{mobile phone, floppy disk})
\end{itemize}
\end{frame}

%------------------------------------------------


\begin{frame}[fragile] % Need to use the fragile option when verbatim is used in the slide
\frametitle{Conclusion}
\begin{itemize}
    \item Despite their impressive performance on a range of tasks, recent off-the-shelf embedding models are inferior to simple word2vec in predicting the compositionality of MWEs
    \item Infersent, fastText and ELMo are, among the better performing models and do not require prior knowledge of MWEs
    \item Paraphrases enhance the compositionality predictions of MWEs
\end{itemize} \\~\\
\textbf{Future work} \\
\begin{itemize}
    \item Compare the models' performances when trained on the same corpus and/or applied to other languages
    \item Enhance compositionality predictions using semantic relationship data -- shared learning between the tasks
\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{References}
\tiny
%include in APA format
\begin{columns}[T] % align columns
\begin{column}{.48\textwidth}
Venkatapathy, S., \& Joshi, A. K. (2006). Using information about multi-word expressions for the word-alignment task. In Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties (pp 20--27).\\~\\
Acosta O. C., Villavicencio, A., \& Moreira, V. P. (2011). Identification and treatment of multiword expressions applied to information retrieval. In Proceedings of the ALC Workshop on MWEs: from Parsing and Generation to the Real World (pp 101--109).\\~\\
Salehi, B., Mathur, N., Cook, P., \& Baldwin, T. (2015). The impact of multiword expression compositionality on machine translation evaluation. In Proceedings of the 11th Workshop on Multiword Expressions (pp. 54-59).\\~\\
Salehi, B., Cook, P., \& Baldwin, T. (2015). A Word Embedding Approach to Predicting the Compositionality of Multiword Expressions. HLT-NAACL. \\~\\
Cordeiro, S., Villavicencio, A., Idiart, M., \& Ramisch, C. (2019). Unsupervised Compositionality Prediction of Nominal Compounds. Computational Linguistics (pp 1-57).
\end{column}%
\hfill%
\begin{column}{.48\textwidth}
Reddy, S., McCarthy, D., \& Manandhar, S. (2011). An empirical study on compositionality in compound nouns. In Proceedings of 5th International Joint Conference on Natural Language Processing (pp. 210-218).\\~\\
Ramisch, C., Cordeiro, S., Zilio, L., Idiart, M., \& Villavicencio, A. (2016). How naked is the naked truth? a multilingual lexicon of nominal compound compositionality. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) (Vol. 2, pp. 156-161).\\~\\
Biemann, C., \& Giesbrecht, E. (2011, June). Distributional semantics and compositionality 2011: Shared task description and results. In Proceedings of the workshop on distributional semantics and compositionality (pp. 21-28). Association for Computational Linguistics.\\~\\
Farahmand, M., Smith, A., \& Nivre, J. (2015). A multiword expression data set: Annotating non-compositionality and conventionalization for english noun compounds. In Proceedings of the 11th Workshop on Multiword Expressions (pp. 29-33).
\end{column}%
\end{columns}
\end{frame}

%----------------------------------------------------------------------------------------

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
